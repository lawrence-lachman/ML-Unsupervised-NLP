{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCsWdxb46HTZ"
   },
   "outputs": [],
   "source": [
    "# ! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf5-0bt06HTd"
   },
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy's part-of-speech and dependency tags\n",
    "\n",
    "The list is:\n",
    "\n",
    "* ADJ: adjective\n",
    "* ADP: adposition\n",
    "* ADV: adverb\n",
    "* AUX: auxiliary verb\n",
    "* CONJ: coordinating conjunction\n",
    "* DET: determiner\n",
    "* INTJ: interjection\n",
    "* NOUN: noun\n",
    "* NUM: numeral\n",
    "* PART: particle\n",
    "* PRON: pronoun\n",
    "* PROPN: proper noun\n",
    "* PUNCT: punctuation\n",
    "* SCONJ: subordinating conjunction\n",
    "* SYM: symbol\n",
    "* VERB: verb\n",
    "* X: other\n",
    "\n",
    "https://spacy.io/api/annotation\n",
    "\n",
    "The list of other attributes for tokens can be found at https://spacy.io/api/token\n",
    "\n",
    "## Dependency Tokens\n",
    "\n",
    "spaCy's dependency tag scheme is based upon the ClearNLP project; the meanings of the tags can be found at https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md:\n",
    "\n",
    "* ACL: Clausal modifier of noun\n",
    "* ACOMP: Adjectival complement\n",
    "* ADVCL: Adverbial clause modifier\n",
    "* ADVMOD: Adverbial modifier\n",
    "* AGENT: Agent\n",
    "* AMOD: Adjectival modifier\n",
    "* APPOS: Appositional modifier\n",
    "* ATTR: Attribute\n",
    "* AUX: Auxiliary\n",
    "* AUXPASS: Auxiliary (passive)\n",
    "* CASE: Case marker\n",
    "* CC: Coordinating conjunction\n",
    "* CCOMP: Clausal complement\n",
    "* COMPOUND: Compound modifier\n",
    "* CONJ: Conjunct\n",
    "* CSUBJ: Clausal subject\n",
    "* CSUBJPASS: Clausal subject (passive)\n",
    "* DATIVE: Dative\n",
    "* DEP: Unclassified dependent\n",
    "* DET: Determiner\n",
    "* DOBJ: Direct Object\n",
    "* EXPL: Expletive\n",
    "* INTJ: Interjection\n",
    "* MARK: Marker\n",
    "* META: Meta modifier\n",
    "* NEG: Negation modifier\n",
    "* NOUNMOD: Modifier of nominal\n",
    "* NPMOD: Noun phrase as adverbial modifier\n",
    "* NSUBJ: Nominal subject\n",
    "* NSUBJPASS: Nominal subject (passive)\n",
    "* NUMMOD: Number modifier\n",
    "* OPRD: Object predicate\n",
    "* PARATAXIS: Parataxis\n",
    "* PCOMP: Complement of preposition\n",
    "* POBJ: Object of preposition\n",
    "* POSS: Possession modifier\n",
    "* PRECONJ: Pre-correlative conjunction\n",
    "* PREDET: Pre-determiner\n",
    "* PREP: Prepositional modifier\n",
    "* PRT: Particle\n",
    "* PUNCT: Punctuation\n",
    "* QUANTMOD: Modifier of quantifier\n",
    "* RELCL: Relative clause modifier\n",
    "* ROOT: Root\n",
    "* XCOMP: Open clausal complement\n",
    "\n",
    "https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVGhCPaA6HTh",
    "outputId": "bb45e10b-e425-4533-ec2e-cb069ce13983"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1f4a80472e8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1f4a97a3ee8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1f4a97a3f48>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER and word vectors\n",
    "# • The nlp object is used to create documents, access linguistic annotations and different nlp properties\n",
    "# • en_core_web_sm is an statistical model of the English class with the model weights loaded in, and built-in pipeline, so\n",
    "#   spaCy can predict part-of-speech tags, dependency labels and named entities \n",
    "# • noun_chunks and dependency tagging requires the dependency parser and tagger, respectfully\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition \n",
    "\n",
    "https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "| TYPE | DESCRIPTION |\n",
    "|-----|-----|\n",
    "| PERSON | People, including fictional  |\n",
    "| NORP | Nationalities or religious or political groups |\n",
    "| FAC |\tBuildings, airports, highways, bridges, etc.  |\n",
    "| ORG |\tCompanies, agencies, institutions, etc. |\n",
    "| GPE |\tCountries, cities, states |\n",
    "| LOC |\tNon-GPE locations, mountain ranges, bodies of water |\n",
    "| PRODUCT |\tObjects, vehicles, foods, etc. (Not services.) |\n",
    "| EVENT | Named hurricanes,  battles, wars, sports events, etc. |\n",
    "| WORK_OF_ART |\tTitles of books, songs, etc. |\n",
    "| LAW |\tNamed documents made into laws |\n",
    "| LANGUAGE | Any named language |\n",
    "| DATE | Absolute or relative dates or periods |\n",
    "| TIME | Times smaller than a day |\n",
    "| PERCENT |\tPercentage, including ”%“ |\n",
    "| MONEY | Monetary values, including unit |\n",
    "| QUANTITY | Measurements, as of weight or distance |\n",
    "| ORDINAL |\t“first”, “second”, etc. |\n",
    "| CARDINAL | Numerals that do not fall under another type |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'be', 'talk', 'say']\n",
      "\n",
      "Entities:\n",
      "\t Sebastian Thrun PERSON\n",
      "\t Google ORG\n",
      "\t 2007 DATE\n",
      "\t American NORP\n",
      "\t Thrun PERSON\n",
      "\t Recode ORG\n",
      "\t earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# Process whole documents\n",
    "text = ('When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took \\\n",
    "him seriously. “I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I \\\n",
    "wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week.')\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"\\nVerbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "print('\\nEntities:')\n",
    "# Find named entities, phrases and concepts (places, people, organizations, and languages)\n",
    "for entity in doc.ents:\n",
    "    print('\\t', entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification in Python Using spaCy\n",
    "\n",
    "<span style=\"color:red\">**Important Article**:  </span>[Tutorial: Text Classification in Python Using spaCy](https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsubj: nominal subject\n",
      "dobj: direct object\n",
      "pobj: object of preposition\n"
     ]
    }
   ],
   "source": [
    "# To get the meaning of a dependency tag, use explain()\n",
    "print('nsubj:', spacy.explain('nsubj'))\n",
    "print('dobj:', spacy.explain('dobj'))\n",
    "print('pobj:', spacy.explain('pobj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pursuit pursuit pobj In\n",
      "a wall wall pobj of\n",
      "President Trump Trump nsubj ran\n"
     ]
    }
   ],
   "source": [
    "docp = nlp('In pursuit of a wall, President Trump ran into one.')\n",
    "\n",
    "for chunk in docp.noun_chunks:\n",
    "   print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a6b51dff45814baf803882a4c558e86e-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">In</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">pursuit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">wall,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">President</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Trump</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">ran</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">into</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">one.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-1\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M215.0,266.5 L223.0,254.5 207.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a6b51dff45814baf803882a4c558e86e-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a6b51dff45814baf803882a4c558e86e-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,266.5 L1623.0,254.5 1607.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For visualization of Entity detection importing displacy from spaCy\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(docp, style = \"dep\", jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(New York City, 'GPE', 384),\n",
       " (Tuesday, 'DATE', 391),\n",
       " (At least 285, 'CARDINAL', 397),\n",
       " (September, 'DATE', 391),\n",
       " (Brooklyn, 'GPE', 384),\n",
       " (Williamsburg, 'GPE', 384),\n",
       " (four, 'CARDINAL', 397),\n",
       " (Bill de Blasio, 'PERSON', 380),\n",
       " (Tuesday, 'DATE', 391),\n",
       " (Orthodox Jews, 'NORP', 381),\n",
       " (6 months old, 'DATE', 391),\n",
       " (up to $1,000, 'MONEY', 394)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nytimes = nlp(u'New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an \\\n",
    "outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.  At least 285 people \\\n",
    "have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip \\\n",
    "codes there, Mayor Bill de Blasio (D) said Tuesday.  The mandate orders all unvaccinated people in the area, including a \\\n",
    "concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old.  Anyone who resists \\\n",
    "could be fined up to $1,000.')\n",
    "\n",
    "entities=[(i, i.label_, i.label) for i in nytimes.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    New York City\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.  \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    At least 285\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people have contracted measles in the city since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    September\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", mostly in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Brooklyn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Williamsburg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " neighborhood. The order covers \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    four\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " Zip codes there, Mayor \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Bill de Blasio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (D) said \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".  The mandate orders all unvaccinated people in the area, including a concentration of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Orthodox Jews\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", to receive inoculations, including for children as young as \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    6 months old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".  Anyone who resists could be fined \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    up to $1,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nytimes, style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Representation\n",
    "\n",
    "Using spaCy‘s en_core_web_sm model, let’s take a look at the length of a vector for a single word, and what that vector \n",
    "looks like using .vector and .shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "[ 1.0466383  -1.5323697  -0.72177905 -2.4700649  -0.2715162   1.1589639\n",
      "  1.7113379  -0.31615403 -2.0978343   1.837553    1.4681302   2.728043\n",
      " -2.3457408  -5.17184    -4.6110015  -0.21236466 -0.3029521   4.220028\n",
      " -0.6813917   2.4016762  -1.9546705  -0.85086954  1.2456163   1.5107994\n",
      "  0.4684736   3.1612053   0.15542296  2.0598564   3.780035    4.6110964\n",
      "  0.6375268  -1.078107   -0.96647096 -1.3939928  -0.56914186  0.51434743\n",
      "  2.3150034  -0.93199825 -2.7970662  -0.8540115  -3.4250052   4.2857723\n",
      "  2.5058174  -2.2150877   0.7860181   3.496335   -0.62606215 -2.0213525\n",
      " -4.47421     1.6821622  -6.0789204   0.22800982 -0.36950028 -4.5340714\n",
      " -1.7978683  -2.080299    4.125556    3.1852438  -3.286446    1.0892276\n",
      "  1.017115    1.2736416  -0.10613725  3.5102775   1.1902348   0.05483437\n",
      " -0.06298041  0.8280688   0.05514218  0.94817173 -0.49377063  1.1512338\n",
      " -0.81374085 -1.6104267   1.8233354  -2.278403   -2.1321895   0.3029334\n",
      " -1.4510616  -1.0584296  -3.5698352  -0.13046083 -0.2668339   1.7826645\n",
      "  0.4639858  -0.8389523  -0.02689964  2.316218    5.8155413  -0.45935947\n",
      "  4.368636    1.6603007  -3.1823301  -1.4959551  -0.5229269   1.3637555 ]\n"
     ]
    }
   ],
   "source": [
    "mango = nlp(u'mango')\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification With Machine Learning and SpaCy\n",
    "\n",
    "[Amazon Alexa Reviews](https://www.kaggle.com/sid321axn/amazon-alexa-reviews/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 5) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      "rating              3150 non-null int64\n",
      "date                3150 non-null object\n",
      "variation           3150 non-null object\n",
      "verified_reviews    3150 non-null object\n",
      "feedback            3150 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  Sometimes while playing a game, you can answer...         1  \n",
       "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading TSV file\n",
    "df_amazon = pd.read_csv (\"../data/amazon_alexa.tsv\", sep = \"\\t\")\n",
    "print(df_amazon.shape, '\\n')\n",
    "print(df_amazon.info())\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.918413\n",
       "0    0.081587\n",
       "Name: feedback, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.feedback.value_counts()/df_amazon.feedback.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokening the Data With spaCy\n",
    "\n",
    "Example where nlp = spacy.load(\"en_core_web_sm\"), while parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGjj0z3v6HTm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# https://stackoverflow.com/questions/51072516/pos-in-spacy-is-not-returning-any-results-in-python\n",
    "#\n",
    "# Load the English language class dictionary, which includes the language-specific data like tokenization rules, but doesn't \n",
    "# actually load a model, which enables spaCy to predict part-of-speech tags and other linguistic annotations.  It contains the\n",
    "# language data and tokenizer, but doesn't have a statistical model.  This means that spaCy will tokenize the text, but its \n",
    "# pipeline is empty; it doesn't include a tagger or parser, and it doesn't have any weights or attributes available either\n",
    "So the  will be empty\n",
    "parser = English()\n",
    "parser.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "# Process text.  The nlp object is used to create documents with linguistic annotations\n",
    "doc = parser(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['beside',\n",
       " 'yourself',\n",
       " 'formerly',\n",
       " 'him',\n",
       " 'often',\n",
       " 'own',\n",
       " 'themselves',\n",
       " 'seems',\n",
       " 'front',\n",
       " 're',\n",
       " 'eight',\n",
       " '‘ve',\n",
       " 'mine',\n",
       " 'n’t',\n",
       " 'three',\n",
       " 'upon',\n",
       " 'beforehand',\n",
       " 'one',\n",
       " 'have',\n",
       " 'next',\n",
       " 'still',\n",
       " 'in',\n",
       " 'other',\n",
       " 'out',\n",
       " 'once',\n",
       " 'under',\n",
       " 'might',\n",
       " 'she',\n",
       " 'be',\n",
       " 'nowhere',\n",
       " '‘ll',\n",
       " 'sixty',\n",
       " 'make',\n",
       " 'as',\n",
       " 'become',\n",
       " 'everything',\n",
       " 'rather',\n",
       " '’m',\n",
       " 'others',\n",
       " 'former',\n",
       " 'our',\n",
       " 'towards',\n",
       " 'see',\n",
       " 'give',\n",
       " 'on',\n",
       " 'its',\n",
       " 'above',\n",
       " 'we',\n",
       " 'seemed',\n",
       " 'around',\n",
       " '’re',\n",
       " 'something',\n",
       " \"'s\",\n",
       " 'how',\n",
       " 'therein',\n",
       " 'below',\n",
       " 'do',\n",
       " 'thereupon',\n",
       " 'became',\n",
       " 'anywhere',\n",
       " 'am',\n",
       " 'though',\n",
       " 'among',\n",
       " 'anyone',\n",
       " 'call',\n",
       " 'third',\n",
       " 'many',\n",
       " 'somehow',\n",
       " 'say',\n",
       " 'been',\n",
       " 'not',\n",
       " 'quite',\n",
       " 'thus',\n",
       " 'was',\n",
       " 'put',\n",
       " 'regarding',\n",
       " 'her',\n",
       " 'is',\n",
       " 'were',\n",
       " 'even',\n",
       " 'whom',\n",
       " 'amount',\n",
       " 'the',\n",
       " 'none',\n",
       " 'whereby',\n",
       " 'before',\n",
       " 'a',\n",
       " 'enough',\n",
       " 'his',\n",
       " 'perhaps',\n",
       " 'until',\n",
       " 'bottom',\n",
       " 'who',\n",
       " 'against',\n",
       " 'hereby',\n",
       " 'when',\n",
       " 'whereas',\n",
       " 'your',\n",
       " 'us',\n",
       " 'five',\n",
       " 'onto',\n",
       " 'get',\n",
       " 'last',\n",
       " 'any',\n",
       " 'than',\n",
       " 'and',\n",
       " 'now',\n",
       " 'elsewhere',\n",
       " 'very',\n",
       " 'but',\n",
       " '‘s',\n",
       " 'nor',\n",
       " 'using',\n",
       " 'same',\n",
       " 'more',\n",
       " 'made',\n",
       " 'herein',\n",
       " 'noone',\n",
       " 'twenty',\n",
       " 'these',\n",
       " 'down',\n",
       " 'therefore',\n",
       " 'could',\n",
       " 'except',\n",
       " 'most',\n",
       " 'whereupon',\n",
       " \"'d\",\n",
       " 'twelve',\n",
       " 'thereby',\n",
       " 'everyone',\n",
       " \"'ve\",\n",
       " 'up',\n",
       " 'n‘t',\n",
       " 'so',\n",
       " 'already',\n",
       " 'go',\n",
       " 'four',\n",
       " 'from',\n",
       " '’ve',\n",
       " 'first',\n",
       " 'eleven',\n",
       " 'did',\n",
       " 'just',\n",
       " 'alone',\n",
       " 'besides',\n",
       " 'becomes',\n",
       " 'you',\n",
       " 'has',\n",
       " 'per',\n",
       " 'whatever',\n",
       " 'nine',\n",
       " \"'ll\",\n",
       " 'being',\n",
       " 'by',\n",
       " 'to',\n",
       " '’d',\n",
       " 'everywhere',\n",
       " 'along',\n",
       " \"n't\",\n",
       " 'here',\n",
       " 'someone',\n",
       " 'either',\n",
       " 'no',\n",
       " 'every',\n",
       " '‘d',\n",
       " 'because',\n",
       " 'always',\n",
       " 'neither',\n",
       " 'of',\n",
       " 'about',\n",
       " 'nothing',\n",
       " 'much',\n",
       " 'fifteen',\n",
       " 'must',\n",
       " 'hundred',\n",
       " 'cannot',\n",
       " 'they',\n",
       " 'becoming',\n",
       " 'would',\n",
       " 'full',\n",
       " 'several',\n",
       " 'yourselves',\n",
       " 'since',\n",
       " 'never',\n",
       " 'beyond',\n",
       " 'back',\n",
       " 'somewhere',\n",
       " 'ourselves',\n",
       " 'move',\n",
       " 'ca',\n",
       " 'yours',\n",
       " 'otherwise',\n",
       " 'into',\n",
       " 'through',\n",
       " 'without',\n",
       " 'keep',\n",
       " 'two',\n",
       " 'empty',\n",
       " 'whence',\n",
       " 'hence',\n",
       " 'nobody',\n",
       " '‘re',\n",
       " 'latter',\n",
       " 'mostly',\n",
       " 'then',\n",
       " 'fifty',\n",
       " 'what',\n",
       " 'afterwards',\n",
       " 'it',\n",
       " 'toward',\n",
       " 'used',\n",
       " 'although',\n",
       " 'does',\n",
       " 'thence',\n",
       " 'behind',\n",
       " 'too',\n",
       " 'forty',\n",
       " 'there',\n",
       " 'such',\n",
       " 'amongst',\n",
       " 'after',\n",
       " 'seem',\n",
       " 'however',\n",
       " 'part',\n",
       " 'why',\n",
       " 'off',\n",
       " \"'re\",\n",
       " 'had',\n",
       " 'name',\n",
       " 'throughout',\n",
       " 'seeming',\n",
       " 'my',\n",
       " 'take',\n",
       " 'anyway',\n",
       " 'myself',\n",
       " 'top',\n",
       " 'whenever',\n",
       " 'with',\n",
       " 'at',\n",
       " '’ll',\n",
       " 'both',\n",
       " 'anyhow',\n",
       " 'done',\n",
       " 'few',\n",
       " 'only',\n",
       " 'where',\n",
       " 'i',\n",
       " 'whither',\n",
       " 'please',\n",
       " 'almost',\n",
       " 'their',\n",
       " 'latterly',\n",
       " 'indeed',\n",
       " 'namely',\n",
       " 'well',\n",
       " 'within',\n",
       " 'if',\n",
       " 'may',\n",
       " 'over',\n",
       " 'less',\n",
       " 'whether',\n",
       " 'ours',\n",
       " 'side',\n",
       " '’s',\n",
       " 'sometimes',\n",
       " 'ever',\n",
       " 'six',\n",
       " 'them',\n",
       " 'will',\n",
       " 'some',\n",
       " 'are',\n",
       " 'me',\n",
       " 'those',\n",
       " 'ten',\n",
       " 'another',\n",
       " 'across',\n",
       " 'all',\n",
       " 'between',\n",
       " 'least',\n",
       " 'whereafter',\n",
       " 'hereupon',\n",
       " 'that',\n",
       " 'should',\n",
       " 'wherein',\n",
       " 'yet',\n",
       " 'various',\n",
       " 'herself',\n",
       " 'serious',\n",
       " 'anything',\n",
       " 'whole',\n",
       " 'an',\n",
       " 'each',\n",
       " 'doing',\n",
       " 'for',\n",
       " 'hereafter',\n",
       " 'he',\n",
       " 'whose',\n",
       " 'together',\n",
       " 'this',\n",
       " 'again',\n",
       " 'during',\n",
       " '‘m',\n",
       " 'nevertheless',\n",
       " 'really',\n",
       " \"'m\",\n",
       " 'show',\n",
       " 'thereafter',\n",
       " 'moreover',\n",
       " 'also',\n",
       " 'itself',\n",
       " 'meanwhile',\n",
       " 'himself',\n",
       " 'whoever',\n",
       " 'further',\n",
       " 'due',\n",
       " 'wherever',\n",
       " 'can',\n",
       " 'sometime',\n",
       " 'hers',\n",
       " 'unless',\n",
       " 'else',\n",
       " 'thru',\n",
       " 'which',\n",
       " 'while',\n",
       " 'or',\n",
       " 'via']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Build a list of stopwords to use to filter\n",
    "stop_words = list(STOP_WORDS)\n",
    "print('Number of stop words: %d' % len(stop_words))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy tokenizer that accepts a sentence as input and processes the sentence into tokens (which can also be replaced \n",
    "# by word vectors), performing lemmatization, lowercasing, and removing stop words and punctuation \n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating token object, which is used to create documents with linguistic annotations\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words and punctuation\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'walker', 'wall', 'sit', 'run', 'runner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = 'He was walking with the walker in the Wall; he may had sat and run with the runner, too'\n",
    "spacy_tokenizer(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Transformer\n",
    "\n",
    "To further clean our text data, we’ll also want to create a custom transformer for removing initial and end spaces and \n",
    "converting text into lower case.  Here, we create a custom predictors class which inherits the TransformerMixin class. \n",
    "This class overrides the transform, fit and get_parrams methods.  We also create a clean_text() function that removes \n",
    "spaces and converts text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Feature Engineering (TF-IDF)\n",
    "\n",
    "To represent our text numerically, convert text into the matrix of occurrence of words within a given document. It generates \n",
    "a matrix that we refer to as a Bag of Words matrix or a document term matrix (DTM).  Generate a DTM by using scikit-learn‘s CountVectorizer.  In the code below, CountVectorizer uses the custom spacy_tokenizer function as its tokenizer, and define the ngram range we want (combinations of adjacent words).\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) – simply a way of normalizing our Bag of Words by looking at each word’s frequency in comparison to the document frequency; how important a particular term is based on how many times the term appears and how many other documents that same term appears in.  The higher the TF-IDF, the more important that term is to that document.  We can represent this with the following mathematical equation:\n",
    "\n",
    "![tfidf equation](../data/tfidf_eqn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline and Generating the Model\n",
    "\n",
    "Create a pipeline with three components: a cleaner, a vectorizer, and a classifier. \n",
    "* The cleaner uses our predictors class object to clean and preprocess the text\n",
    "* The vectorizer uses countvector objects to create the bag of words matrix for our text\n",
    "* The classifier is an object that performs the logistic regression to classify the sentiments\n",
    "\n",
    "Once this pipeline is built, we’ll fit the pipeline components using fit().\n",
    "\n",
    "[How Pipelines Work](https://spacy.io/usage/processing-pipelines/#pipeline)\n",
    "\n",
    "### Built-in pipeline components\n",
    "\n",
    " |  STRING NAME |\tCOMPONENT | DESCRIPTION\n",
    "|-----------|----------|----------|\n",
    "| tagger | Tagger |\tAssign part-of-speech-tags |\n",
    "| parser | DependencyParser | Assign dependency labels |\n",
    "| ner | EntityRecognizer | Assign named entities |\n",
    "| textcat | TextCategorizer | Assign text categories |\n",
    "| entity_ruler | EntityRuler | Assign named entities based on pattern rules |\n",
    "| sentencizer | Sentencizer\t | Add rule-based sentence segmentation without the dependency parse |\n",
    "| merge_noun_chunks | merge_noun_chunks | Merge all noun chunks into a single token. Should be added after the tagger and parser |\n",
    "| merge_entities | merge_entities | Merge all entities into a single token. Should be added after the entity recognizer |\n",
    "| merge_subtokens | merge_subtokens | Merge subtokens predicted by the parser into single tokens. Should be added after the parser |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\.conda\\envs\\dsi03-inference\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x000001F4AECC6080>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "    ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Use various functions of the metrics module to look at our model’s accuracy, precision, and recall.\n",
    "\n",
    "* [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) refers to the percentage of the total predictions our model makes that are completely correct.\n",
    "* [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) describes the ratio of true positives to true positives plus false positives in our predictions.\n",
    "* [Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) describes the ratio of true positives to true positives plus false negatives in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9164021164021164\n",
      "Logistic Regression Precision: 0.9162248144220573\n",
      "Logistic Regression Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\", metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\", metrics.recall_score(y_test, predicted))\n",
    "\n",
    "# • Our model correctly identified a comment’s sentiment 94.1% of the time\n",
    "# • When it predicted a review was positive, that review was actually positive 95% of the time\n",
    "# • When handed a positive review, our model identified it as positive 98.6% of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Lemma\n",
    "\n",
    "https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/Text%20Classification%20With%20Machine%20Learning,SpaCy,Sklearn(Sentiment%20Analysis)/Text%20Classification%20&%20Sentiment%20Analysis%20with%20SpaCy,Sklearn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = This | lemma =  This\n",
      "word = is | lemma =  be\n",
      "word = how | lemma =  how\n",
      "word = Johnny | lemma =  Johnny\n",
      "word = Walker | lemma =  Walker\n",
      "word = was | lemma =  be\n",
      "word = walking | lemma =  walk\n",
      "word = . | lemma =  .\n",
      "word =   | lemma =   \n",
      "word = He | lemma =  He\n",
      "word = was | lemma =  be\n",
      "word = also | lemma =  also\n",
      "word = running | lemma =  run\n",
      "word = beside | lemma =  beside\n",
      "word = the | lemma =  the\n",
      "word = lawn | lemma =  lawn\n",
      "word = . | lemma =  .\n"
     ]
    }
   ],
   "source": [
    "docx = nlp('This is how Johnny Walker was walking.  He was also running beside the lawn.')\n",
    "\n",
    "# Lemmatizing of tokens\n",
    "for word in docx:\n",
    "    print('word =', word.text, \"| lemma = \", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "learn\n",
      "datum\n",
      "science\n",
      ",\n",
      "you\n",
      "should\n",
      "not\n",
      "get\n",
      "discourage\n",
      "!\n",
      "\n",
      "challenges\n",
      "and\n",
      "setback\n",
      "be\n",
      "not\n",
      "failure\n",
      ",\n",
      "be\n",
      "just\n",
      "part\n",
      "of\n",
      "the\n",
      "journey\n",
      ".\n",
      "have\n",
      "get\n",
      "this\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Lemma that are not pronouns\n",
    "for word in docx:\n",
    "    if word.lemma_ != '-PRON-':\n",
    "        print(word.lemma_.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'learn',\n",
       " 'datum',\n",
       " 'science',\n",
       " ',',\n",
       " 'you',\n",
       " 'should',\n",
       " 'not',\n",
       " 'get',\n",
       " 'discourage',\n",
       " '!',\n",
       " '',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'setback',\n",
       " 'be',\n",
       " 'not',\n",
       " 'failure',\n",
       " ',',\n",
       " 'they',\n",
       " 'be',\n",
       " 'just',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'journey',\n",
       " '.',\n",
       " 'you',\n",
       " 'have',\n",
       " 'get',\n",
       " 'this',\n",
       " '!']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List Comprehensions of our Lemma\n",
    "[word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in docx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n",
      "data\n",
      "science\n",
      "discouraged\n",
      " \n",
      "Challenges\n",
      "setbacks\n",
      "failures\n",
      "journey\n",
      "got\n"
     ]
    }
   ],
   "source": [
    "# Filtering out Stopwords and Punctuations\n",
    "for word in docx:\n",
    "    if word.is_stop == False and not word.is_punct:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[learning,\n",
       " data,\n",
       " science,\n",
       " discouraged,\n",
       "  ,\n",
       " Challenges,\n",
       " setbacks,\n",
       " failures,\n",
       " journey,\n",
       " got]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words and Punctuation In list Comprehension\n",
    "[word for word in docx if word.is_stop == False and not word.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', ' ', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"When learning data science, you shouldn't get discouraged!  Challenges and setbacks aren't failures, they're \\\n",
    "just part of the journey. You've got this!\"\n",
    "\n",
    "docx = nlp(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in docx:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)\n",
    "\n",
    "# spaCy recognizes that contractions such as shouldn’t, they're, and aren't actually represent two distinct words, and it has \n",
    "# thus broken them down into two distinct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All \n",
      "is \n",
      "well \n",
      "that \n",
      "ends \n",
      "well \n",
      ". \n",
      "6\n"
     ]
    }
   ],
   "source": [
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text, word.pos_)\n",
    " \n",
    "# Count the number of words in the sentence, excluding only punctuation marks\n",
    "mytokens = [word for word in docs if not word.is_punct]\n",
    "print(len(mytokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When learning data science, you shouldn't get discouraged!\", \" Challenges and setbacks aren't failures, they're just part of the journey.\", \"You've got this!\"]\n"
     ]
    }
   ],
   "source": [
    "# Start over with the English parser that does not contain a pipeline, and thus doesn't contain a sentencizer\n",
    "\n",
    "# By default, the English spaCy parser breaks the text into tokens.  We can also break the text into sentences rather than \n",
    "# words - sentence tokenization - where the tokenizer looks for specific characters that fall between sentences, like periods, \n",
    "# exclaimation points, and newline characters. Sentence tokenization requires a preprocessing pipeline because sentence \n",
    "# preprocessing using spaCy includes a tokenizer, a tagger, a parser and an entity recognizer to correctly identify what’s a \n",
    "# sentence and what isn’t\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = parser.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "parser.add_pipe(sbd)\n",
    "\n",
    "docx = parser(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "for sent in docx.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Parameter Search Function\n",
    "\n",
    "* This function finds the parameters that produce# the highest Normalized Mutual Infomation score from our clusters. This score is a good baseline from which to compare clustering vs classification because it correlates with good clutering as well as higher accuracy scores.\n",
    "* It prints the relevant statistics as well as a contingency matrix of the result and stores the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a 2d PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "print(result.shape)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = df['url'].str.replace('foodnewsfeed', 'fsrmagazine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "03_spacy_test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
